{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "929835b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocession process: 0\n",
      "Preprocession process: 10000\n",
      "Preprocession process: 20000\n",
      "Preprocession process: 30000\n",
      "Preprocession process: 40000\n",
      "Preprocession process: 50000\n",
      "Preprocession process: 60000\n",
      "Preprocession process: 70000\n",
      "Preprocession process: 80000\n",
      "Preprocession process: 90000\n",
      "Preprocession process: 100000\n",
      "Preprocession process: 110000\n",
      "Preprocession process: 120000\n",
      "Preprocession process: 130000\n",
      "Preprocession process: 140000\n",
      "Preprocession process: 150000\n",
      "Preprocession process: 160000\n",
      "Preprocession process: 170000\n",
      "Preprocession process: 180000\n",
      "Preprocession process: 190000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-da8a2f573958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mmodel_LR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mmodel_LR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_LR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"liblinear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sag\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"saga\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         )\n\u001b[0;32m-> 1146\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     ]:\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "### A list of dirty words\n",
    "dirtyWordFile = pd.read_csv('../CommonCode/scratch/profanity_words.csv', error_bad_lines='skip')\n",
    "workaroundDirtyWords = dirtyWordFile['text'].values.tolist()\n",
    "canonicalDirtyWords = dirtyWordFile['canonical_form_1'].values.tolist()\n",
    "# a dictionary that maps workaround dirty words to canonical forms, later substitute these words\n",
    "dirtyDict = {}\n",
    "dirtyDict = {workaroundDirtyWords[i]: canonicalDirtyWords[i] for i in range(len(workaroundDirtyWords))}\n",
    "\n",
    "\n",
    "dfs = list()\n",
    "for filename in os.listdir(\"osf_labelled_data/\"):\n",
    "    with open(os.path.join(\"osf_labelled_data\", filename), 'r') as f:\n",
    "        data = pd.read_csv(f)\n",
    "        dfs.append(data)\n",
    "        \n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df = df[['Message', 'Sentiment']]\n",
    "\n",
    "def PreProcessOneMessage(s):\n",
    "    # remove unicode characters\n",
    "    s = s.encode(\"ascii\", \"ignore\").decode()\n",
    "    # remove punctuations\n",
    "    # not removing : for it's a emoji pattern we are about to handle in the loop\n",
    "    # not removing _ for it relates to emoji patterns\n",
    "    s = re.sub('[,.?!@#*/~`$%^&()<>\\-+=;\\'\\\"]', '', s)\n",
    "    slist = s.split()\n",
    "    newstr = \"\"\n",
    "    for i in slist:\n",
    "        # order of if matters\n",
    "        # substitute workaround profane words with canonical forms\n",
    "        if i in dirtyDict:\n",
    "            i = dirtyDict[i]\n",
    "        # remove emoji words in format of \":smiling_face_with_smiling_eyes:\"\n",
    "        if i[0] != ':':\n",
    "            newstr += i\n",
    "            newstr += \" \"\n",
    "    # remove contractions\n",
    "    newstr = contractions.fix(newstr)\n",
    "\n",
    "    #remove stop words\n",
    "    retOneSentence = \"\"\n",
    "    # tokenize\n",
    "    wordTokens = word_tokenize(newstr)\n",
    "    for w in wordTokens:\n",
    "        if w not in stopWords:\n",
    "            # only keep non-stopword words\n",
    "            retOneSentence += w + \" \"\n",
    "    # remove tail space\n",
    "    tempStr = retOneSentence[:-1]\n",
    "    # lemmatization\n",
    "    resultMessage = lemmatizer.lemmatize(tempStr)\n",
    "    return resultMessage\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 10000 == 0:\n",
    "        print(\"Preprocession process: {}\".format(index))\n",
    "    df.loc[index, 'Message'] = PreProcessOneMessage(row['Message'])\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['Sentiment'] > 0.6:\n",
    "        df.loc[index, 'Sentiment'] = 1\n",
    "    else:\n",
    "        df.loc[index, 'Sentiment'] = 0\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(df['Message'])\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Message'], df['Sentiment'], test_size=0.2)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_LR = LogisticRegression()\n",
    "model_LR.fit(vectorizer.transform(X_train), y_train)\n",
    "\n",
    "y_pred = model_LR.predict(vectorizer.transform(X_test))\n",
    "\n",
    "count = 0\n",
    "for index, i in enumerate(y_test):\n",
    "    if i != y_pred[index]:\n",
    "        count = count + 1\n",
    "count / len(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
